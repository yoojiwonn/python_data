{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoojiwonn/python_data/blob/main/keras_DNN_iris_YJW_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYWPrHpHvOaT",
        "outputId": "d55f0939-d001-49db-ba22-3642fe645da8"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')   # google drive를 google colab에 연결. 처음 실행 시, 인증 필요 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22UZuhWkLtVt"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers  # 모듈(변수나 함수를 포함)만 불러오기\n",
        "\n",
        "# BMI 데이터를 읽어 들이고 정규화하기\n",
        "dir = \"/content/gdrive/My Drive/Colab Notebooks/ai/\"  ### [중요!!] Colab의 경우, bmi.csv를 저장할 위치를 기록하시오.\n",
        "df = pd.read_csv(dir + \"iris.csv\")    ### [중요!!] https://github.com/jjyjung/ai/blob/gh-pages/bmi.csv 에서 다운받을 수 있습니다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GAFF1QOk58E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "b7f14550-81a0-4094-ffcd-d09543c5365f"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sepal_length  sepal_width  petal_length  petal_width        iris_type\n",
              "0             6.4          3.1           5.5          1.8   Iris-virginica\n",
              "1             6.5          3.0           5.8          2.2   Iris-virginica\n",
              "2             4.6          3.1           1.5          0.2      Iris-setosa\n",
              "3             6.4          2.8           5.6          2.1   Iris-virginica\n",
              "4             5.0          3.3           1.4          0.2      Iris-setosa\n",
              "..            ...          ...           ...          ...              ...\n",
              "145           5.1          3.8           1.9          0.4      Iris-setosa\n",
              "146           5.7          2.8           4.5          1.3  Iris-versicolor\n",
              "147           6.9          3.1           5.4          2.1   Iris-virginica\n",
              "148           7.2          3.0           5.8          1.6   Iris-virginica\n",
              "149           4.9          3.0           1.4          0.2      Iris-setosa\n",
              "\n",
              "[150 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d7d24ccf-b56c-4458-97ac-033b31c049e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>iris_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.4</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.5</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>2.2</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.4</td>\n",
              "      <td>2.8</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.1</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.3</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>5.7</td>\n",
              "      <td>2.8</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1.3</td>\n",
              "      <td>Iris-versicolor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.1</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>7.2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1.6</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7d24ccf-b56c-4458-97ac-033b31c049e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d7d24ccf-b56c-4458-97ac-033b31c049e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d7d24ccf-b56c-4458-97ac-033b31c049e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaC6EJH_kwwz"
      },
      "source": [
        "# 몸무게와 키 데이터\n",
        "df[\"sepal_length\"] /= 100   # normalization\n",
        "df[\"sepal_width\"] /= 200   # normalization\n",
        "df[\"petal_length\"] /= 200   # normalization\n",
        "df[\"petal_width\"] /= 200   # normalization\n",
        "# X = df[[\"sepal_lenth\", \"sepal_width\",\"petal_length\",\"petal_width\"]].to_numpy()\n",
        "X = df.iloc[:,0:4]\n",
        "df\n",
        "# 레이블링, 라벨링 (labelling) => one-hot encoding\n",
        "bclass = {\"Iris-virginica\":[1,0,0], \"Iris-setosa\":[0,1,0], \"Iris-versicolor\":[0,0,1]}\n",
        "y = np.empty((150,3))     # 150x3 크기의 다차원 벡터 생성\n",
        "for i, v in enumerate(df[\"iris_type\"]):\n",
        "    y[i] = bclass[v]        # \"Iris-virginica\"이면, y[i]=[1,0,0] 와 같이 할당\n",
        "    \n",
        "# 훈련 전용 데이터와 테스트 전용 데이터로 나누기\n",
        "X_train, y_train = X[0:100], y[0:100]\n",
        "X_test,  y_test  = X[100:150], y[100:150]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "G2UgYhccLtVu"
      },
      "source": [
        "# 모델 구조 정의하기\n",
        "model = tf.keras.Sequential()         # 순차적 계층화 준비\n",
        "model.add(layers.Dense(9, input_shape=(4,)))  # 입력 4개로부터 전달받는 9개 노드의 layer 생성\n",
        "model.add(layers.Activation('relu'))  # ReLU 활성화함수 채택\n",
        "model.add(layers.Dropout(0.1))        # dropout ratio=10% (배치 훈련시 10% arc 무시)\n",
        "\n",
        "model.add(layers.Dense(4))            # 4개 노드의 layer 생성\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "model.add(layers.Dense(3))\n",
        "model.add(layers.Activation('softmax'))# 분류(classification)을 위해 softmax 함수 사용\n",
        "\n",
        "# 모델 구축하기\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',  # 다중 교차엔트로피\n",
        "    optimizer=\"rmsprop\",   # 최적화 기법 중 하나\n",
        "    metrics=['accuracy'])  # 정확도 측정"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2IhHchdXLtVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b954a4-8280-4cd8-b32e-cf5e5f00ec01"
      },
      "source": [
        "# 데이터 훈련하기\n",
        "hist = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=10,     # 3개에 한 번씩 업데이터 실행\n",
        "    epochs=500,          # 훈련 데이터셋을 총 500회 반복 실험. 단, 조기중지될 수 있음\n",
        "    validation_split=0.2,  \n",
        "        #validation data 분할 비율. 즉, 100개 중에서 20%인 20개를 validation용으로 분할\n",
        "    # callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25)],  \n",
        "        #'val_loss'를 monitor하여 감소하면 한 번 더 참고 조기중지\n",
        "    verbose=1)   # 전 과정을 화면에 출력(1) 또는 미출력(0) 모드\n",
        "\n",
        "# 테스트 데이터로 평가하기\n",
        "score = model.evaluate(X_test, y_test)\n",
        "print('test_loss: ', score[0])\n",
        "print('test_acc: ', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "8/8 [==============================] - 1s 20ms/step - loss: 1.0974 - accuracy: 0.3125 - val_loss: 1.1010 - val_accuracy: 0.2500\n",
            "Epoch 2/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0959 - accuracy: 0.3875 - val_loss: 1.1022 - val_accuracy: 0.2500\n",
            "Epoch 3/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0952 - accuracy: 0.4000 - val_loss: 1.1032 - val_accuracy: 0.2500\n",
            "Epoch 4/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0937 - accuracy: 0.3875 - val_loss: 1.1042 - val_accuracy: 0.2500\n",
            "Epoch 5/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0956 - accuracy: 0.3625 - val_loss: 1.1050 - val_accuracy: 0.2500\n",
            "Epoch 6/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0917 - accuracy: 0.3875 - val_loss: 1.1062 - val_accuracy: 0.2500\n",
            "Epoch 7/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.0905 - accuracy: 0.3875 - val_loss: 1.1073 - val_accuracy: 0.2500\n",
            "Epoch 8/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0906 - accuracy: 0.4000 - val_loss: 1.1087 - val_accuracy: 0.2500\n",
            "Epoch 9/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0930 - accuracy: 0.3875 - val_loss: 1.1090 - val_accuracy: 0.2500\n",
            "Epoch 10/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0904 - accuracy: 0.3875 - val_loss: 1.1101 - val_accuracy: 0.2500\n",
            "Epoch 11/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0954 - accuracy: 0.3750 - val_loss: 1.1102 - val_accuracy: 0.2500\n",
            "Epoch 12/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0898 - accuracy: 0.3875 - val_loss: 1.1111 - val_accuracy: 0.2500\n",
            "Epoch 13/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0939 - accuracy: 0.3875 - val_loss: 1.1116 - val_accuracy: 0.2500\n",
            "Epoch 14/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.0893 - accuracy: 0.4000 - val_loss: 1.1137 - val_accuracy: 0.2500\n",
            "Epoch 15/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0898 - accuracy: 0.3875 - val_loss: 1.1136 - val_accuracy: 0.2500\n",
            "Epoch 16/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0904 - accuracy: 0.3875 - val_loss: 1.1142 - val_accuracy: 0.2500\n",
            "Epoch 17/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0968 - accuracy: 0.3875 - val_loss: 1.1142 - val_accuracy: 0.2500\n",
            "Epoch 18/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.0898 - accuracy: 0.3875 - val_loss: 1.1147 - val_accuracy: 0.2500\n",
            "Epoch 19/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0899 - accuracy: 0.3750 - val_loss: 1.1148 - val_accuracy: 0.2500\n",
            "Epoch 20/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0909 - accuracy: 0.3875 - val_loss: 1.1151 - val_accuracy: 0.2500\n",
            "Epoch 21/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0937 - accuracy: 0.3875 - val_loss: 1.1152 - val_accuracy: 0.2500\n",
            "Epoch 22/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0922 - accuracy: 0.3875 - val_loss: 1.1137 - val_accuracy: 0.2500\n",
            "Epoch 23/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0931 - accuracy: 0.3875 - val_loss: 1.1127 - val_accuracy: 0.2500\n",
            "Epoch 24/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0944 - accuracy: 0.3875 - val_loss: 1.1129 - val_accuracy: 0.2500\n",
            "Epoch 25/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0918 - accuracy: 0.3875 - val_loss: 1.1135 - val_accuracy: 0.2500\n",
            "Epoch 26/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0919 - accuracy: 0.3750 - val_loss: 1.1137 - val_accuracy: 0.2500\n",
            "Epoch 27/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0907 - accuracy: 0.3875 - val_loss: 1.1134 - val_accuracy: 0.2500\n",
            "Epoch 28/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0881 - accuracy: 0.3875 - val_loss: 1.1136 - val_accuracy: 0.2500\n",
            "Epoch 29/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0878 - accuracy: 0.4000 - val_loss: 1.1151 - val_accuracy: 0.2500\n",
            "Epoch 30/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0900 - accuracy: 0.3875 - val_loss: 1.1144 - val_accuracy: 0.2500\n",
            "Epoch 31/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0922 - accuracy: 0.3750 - val_loss: 1.1144 - val_accuracy: 0.2500\n",
            "Epoch 32/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.0910 - accuracy: 0.3875 - val_loss: 1.1138 - val_accuracy: 0.2500\n",
            "Epoch 33/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.0960 - accuracy: 0.3875 - val_loss: 1.1129 - val_accuracy: 0.2500\n",
            "Epoch 34/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0890 - accuracy: 0.4000 - val_loss: 1.1154 - val_accuracy: 0.2500\n",
            "Epoch 35/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0904 - accuracy: 0.3875 - val_loss: 1.1147 - val_accuracy: 0.2500\n",
            "Epoch 36/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0959 - accuracy: 0.4000 - val_loss: 1.1134 - val_accuracy: 0.2500\n",
            "Epoch 37/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0960 - accuracy: 0.3875 - val_loss: 1.1129 - val_accuracy: 0.2500\n",
            "Epoch 38/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0887 - accuracy: 0.3875 - val_loss: 1.1133 - val_accuracy: 0.2500\n",
            "Epoch 39/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0867 - accuracy: 0.3875 - val_loss: 1.1147 - val_accuracy: 0.2500\n",
            "Epoch 40/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.0901 - accuracy: 0.3875 - val_loss: 1.1143 - val_accuracy: 0.2500\n",
            "Epoch 41/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0923 - accuracy: 0.4000 - val_loss: 1.1145 - val_accuracy: 0.2500\n",
            "Epoch 42/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0881 - accuracy: 0.3750 - val_loss: 1.1146 - val_accuracy: 0.2500\n",
            "Epoch 43/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0865 - accuracy: 0.4000 - val_loss: 1.1147 - val_accuracy: 0.2500\n",
            "Epoch 44/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0922 - accuracy: 0.3875 - val_loss: 1.1135 - val_accuracy: 0.2500\n",
            "Epoch 45/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0859 - accuracy: 0.3875 - val_loss: 1.1148 - val_accuracy: 0.2500\n",
            "Epoch 46/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0827 - accuracy: 0.3875 - val_loss: 1.1163 - val_accuracy: 0.2500\n",
            "Epoch 47/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0920 - accuracy: 0.3875 - val_loss: 1.1156 - val_accuracy: 0.2500\n",
            "Epoch 48/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0929 - accuracy: 0.3875 - val_loss: 1.1142 - val_accuracy: 0.2500\n",
            "Epoch 49/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0884 - accuracy: 0.3875 - val_loss: 1.1135 - val_accuracy: 0.2500\n",
            "Epoch 50/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0843 - accuracy: 0.3875 - val_loss: 1.1144 - val_accuracy: 0.2500\n",
            "Epoch 51/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0923 - accuracy: 0.4000 - val_loss: 1.1149 - val_accuracy: 0.2500\n",
            "Epoch 52/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0882 - accuracy: 0.3875 - val_loss: 1.1141 - val_accuracy: 0.2500\n",
            "Epoch 53/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0914 - accuracy: 0.3875 - val_loss: 1.1144 - val_accuracy: 0.2500\n",
            "Epoch 54/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.0888 - accuracy: 0.3750 - val_loss: 1.1132 - val_accuracy: 0.2500\n",
            "Epoch 55/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0857 - accuracy: 0.3875 - val_loss: 1.1135 - val_accuracy: 0.2500\n",
            "Epoch 56/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0905 - accuracy: 0.3875 - val_loss: 1.1124 - val_accuracy: 0.2500\n",
            "Epoch 57/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0832 - accuracy: 0.4250 - val_loss: 1.1132 - val_accuracy: 0.2500\n",
            "Epoch 58/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0932 - accuracy: 0.3875 - val_loss: 1.1115 - val_accuracy: 0.2500\n",
            "Epoch 59/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0830 - accuracy: 0.4000 - val_loss: 1.1124 - val_accuracy: 0.2500\n",
            "Epoch 60/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.0849 - accuracy: 0.4000 - val_loss: 1.1121 - val_accuracy: 0.2500\n",
            "Epoch 61/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1.0842 - accuracy: 0.3875 - val_loss: 1.1116 - val_accuracy: 0.2500\n",
            "Epoch 62/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0899 - accuracy: 0.4000 - val_loss: 1.1119 - val_accuracy: 0.2500\n",
            "Epoch 63/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0854 - accuracy: 0.4375 - val_loss: 1.1110 - val_accuracy: 0.2500\n",
            "Epoch 64/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0791 - accuracy: 0.3875 - val_loss: 1.1118 - val_accuracy: 0.2500\n",
            "Epoch 65/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0858 - accuracy: 0.4000 - val_loss: 1.1105 - val_accuracy: 0.2500\n",
            "Epoch 66/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0785 - accuracy: 0.4250 - val_loss: 1.1114 - val_accuracy: 0.2500\n",
            "Epoch 67/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0898 - accuracy: 0.3875 - val_loss: 1.1100 - val_accuracy: 0.2500\n",
            "Epoch 68/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0843 - accuracy: 0.4250 - val_loss: 1.1102 - val_accuracy: 0.2500\n",
            "Epoch 69/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0861 - accuracy: 0.3875 - val_loss: 1.1106 - val_accuracy: 0.2500\n",
            "Epoch 70/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0871 - accuracy: 0.4125 - val_loss: 1.1104 - val_accuracy: 0.2500\n",
            "Epoch 71/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0913 - accuracy: 0.4125 - val_loss: 1.1087 - val_accuracy: 0.2500\n",
            "Epoch 72/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0902 - accuracy: 0.4250 - val_loss: 1.1072 - val_accuracy: 0.2500\n",
            "Epoch 73/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0841 - accuracy: 0.4000 - val_loss: 1.1054 - val_accuracy: 0.2500\n",
            "Epoch 74/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0860 - accuracy: 0.3625 - val_loss: 1.1049 - val_accuracy: 0.2500\n",
            "Epoch 75/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0822 - accuracy: 0.3625 - val_loss: 1.1041 - val_accuracy: 0.2500\n",
            "Epoch 76/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0836 - accuracy: 0.3875 - val_loss: 1.1041 - val_accuracy: 0.2500\n",
            "Epoch 77/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0854 - accuracy: 0.3750 - val_loss: 1.1047 - val_accuracy: 0.2500\n",
            "Epoch 78/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0820 - accuracy: 0.3875 - val_loss: 1.1057 - val_accuracy: 0.2500\n",
            "Epoch 79/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0828 - accuracy: 0.3750 - val_loss: 1.1062 - val_accuracy: 0.2500\n",
            "Epoch 80/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0855 - accuracy: 0.4000 - val_loss: 1.1048 - val_accuracy: 0.2500\n",
            "Epoch 81/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0848 - accuracy: 0.3875 - val_loss: 1.1058 - val_accuracy: 0.2500\n",
            "Epoch 82/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0825 - accuracy: 0.4125 - val_loss: 1.1048 - val_accuracy: 0.2500\n",
            "Epoch 83/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0856 - accuracy: 0.4125 - val_loss: 1.1056 - val_accuracy: 0.2500\n",
            "Epoch 84/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0829 - accuracy: 0.3875 - val_loss: 1.1050 - val_accuracy: 0.2500\n",
            "Epoch 85/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0794 - accuracy: 0.4250 - val_loss: 1.1061 - val_accuracy: 0.2500\n",
            "Epoch 86/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0779 - accuracy: 0.4375 - val_loss: 1.1054 - val_accuracy: 0.2500\n",
            "Epoch 87/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0816 - accuracy: 0.4125 - val_loss: 1.1039 - val_accuracy: 0.2500\n",
            "Epoch 88/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0858 - accuracy: 0.3875 - val_loss: 1.1022 - val_accuracy: 0.2500\n",
            "Epoch 89/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0776 - accuracy: 0.4000 - val_loss: 1.1024 - val_accuracy: 0.2500\n",
            "Epoch 90/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.0751 - accuracy: 0.4125 - val_loss: 1.1025 - val_accuracy: 0.2500\n",
            "Epoch 91/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0809 - accuracy: 0.4125 - val_loss: 1.1014 - val_accuracy: 0.2500\n",
            "Epoch 92/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0806 - accuracy: 0.3875 - val_loss: 1.1002 - val_accuracy: 0.2500\n",
            "Epoch 93/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0807 - accuracy: 0.3875 - val_loss: 1.0995 - val_accuracy: 0.2500\n",
            "Epoch 94/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0794 - accuracy: 0.3625 - val_loss: 1.0980 - val_accuracy: 0.2500\n",
            "Epoch 95/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0791 - accuracy: 0.4125 - val_loss: 1.0977 - val_accuracy: 0.2500\n",
            "Epoch 96/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0757 - accuracy: 0.4000 - val_loss: 1.0988 - val_accuracy: 0.2500\n",
            "Epoch 97/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0785 - accuracy: 0.4000 - val_loss: 1.0981 - val_accuracy: 0.2500\n",
            "Epoch 98/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0773 - accuracy: 0.3875 - val_loss: 1.0977 - val_accuracy: 0.2500\n",
            "Epoch 99/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0818 - accuracy: 0.4000 - val_loss: 1.0954 - val_accuracy: 0.2500\n",
            "Epoch 100/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0683 - accuracy: 0.4625 - val_loss: 1.0969 - val_accuracy: 0.2500\n",
            "Epoch 101/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0781 - accuracy: 0.4375 - val_loss: 1.0962 - val_accuracy: 0.2500\n",
            "Epoch 102/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0729 - accuracy: 0.4250 - val_loss: 1.0957 - val_accuracy: 0.2500\n",
            "Epoch 103/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0828 - accuracy: 0.3875 - val_loss: 1.0914 - val_accuracy: 0.2500\n",
            "Epoch 104/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0744 - accuracy: 0.4375 - val_loss: 1.0907 - val_accuracy: 0.2500\n",
            "Epoch 105/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0781 - accuracy: 0.3750 - val_loss: 1.0887 - val_accuracy: 0.2500\n",
            "Epoch 106/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0794 - accuracy: 0.4125 - val_loss: 1.0882 - val_accuracy: 0.2500\n",
            "Epoch 107/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0721 - accuracy: 0.4000 - val_loss: 1.0870 - val_accuracy: 0.2500\n",
            "Epoch 108/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0706 - accuracy: 0.4500 - val_loss: 1.0897 - val_accuracy: 0.2500\n",
            "Epoch 109/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0743 - accuracy: 0.4000 - val_loss: 1.0903 - val_accuracy: 0.2500\n",
            "Epoch 110/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0684 - accuracy: 0.3875 - val_loss: 1.0889 - val_accuracy: 0.2500\n",
            "Epoch 111/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0648 - accuracy: 0.4625 - val_loss: 1.0896 - val_accuracy: 0.2500\n",
            "Epoch 112/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0696 - accuracy: 0.4250 - val_loss: 1.0887 - val_accuracy: 0.2500\n",
            "Epoch 113/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0645 - accuracy: 0.4250 - val_loss: 1.0880 - val_accuracy: 0.2500\n",
            "Epoch 114/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0766 - accuracy: 0.4250 - val_loss: 1.0866 - val_accuracy: 0.2500\n",
            "Epoch 115/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0624 - accuracy: 0.4375 - val_loss: 1.0863 - val_accuracy: 0.2500\n",
            "Epoch 116/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0643 - accuracy: 0.4125 - val_loss: 1.0832 - val_accuracy: 0.2500\n",
            "Epoch 117/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0649 - accuracy: 0.4250 - val_loss: 1.0833 - val_accuracy: 0.2500\n",
            "Epoch 118/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0622 - accuracy: 0.4250 - val_loss: 1.0832 - val_accuracy: 0.2500\n",
            "Epoch 119/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0665 - accuracy: 0.3875 - val_loss: 1.0815 - val_accuracy: 0.2500\n",
            "Epoch 120/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0584 - accuracy: 0.4500 - val_loss: 1.0790 - val_accuracy: 0.2500\n",
            "Epoch 121/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0675 - accuracy: 0.4375 - val_loss: 1.0766 - val_accuracy: 0.2500\n",
            "Epoch 122/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0559 - accuracy: 0.4750 - val_loss: 1.0765 - val_accuracy: 0.2500\n",
            "Epoch 123/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0561 - accuracy: 0.4500 - val_loss: 1.0747 - val_accuracy: 0.2500\n",
            "Epoch 124/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0542 - accuracy: 0.5125 - val_loss: 1.0762 - val_accuracy: 0.2500\n",
            "Epoch 125/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0656 - accuracy: 0.4250 - val_loss: 1.0730 - val_accuracy: 0.3000\n",
            "Epoch 126/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0581 - accuracy: 0.6000 - val_loss: 1.0723 - val_accuracy: 0.3000\n",
            "Epoch 127/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0563 - accuracy: 0.5875 - val_loss: 1.0729 - val_accuracy: 0.2500\n",
            "Epoch 128/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0648 - accuracy: 0.5125 - val_loss: 1.0706 - val_accuracy: 0.3500\n",
            "Epoch 129/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0520 - accuracy: 0.5125 - val_loss: 1.0688 - val_accuracy: 0.5000\n",
            "Epoch 130/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0556 - accuracy: 0.5500 - val_loss: 1.0696 - val_accuracy: 0.3500\n",
            "Epoch 131/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0581 - accuracy: 0.4375 - val_loss: 1.0680 - val_accuracy: 0.4000\n",
            "Epoch 132/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0492 - accuracy: 0.5500 - val_loss: 1.0668 - val_accuracy: 0.4500\n",
            "Epoch 133/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0502 - accuracy: 0.5500 - val_loss: 1.0669 - val_accuracy: 0.3500\n",
            "Epoch 134/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0468 - accuracy: 0.4875 - val_loss: 1.0694 - val_accuracy: 0.2500\n",
            "Epoch 135/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0412 - accuracy: 0.5000 - val_loss: 1.0662 - val_accuracy: 0.3500\n",
            "Epoch 136/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0466 - accuracy: 0.5000 - val_loss: 1.0636 - val_accuracy: 0.5500\n",
            "Epoch 137/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.0520 - accuracy: 0.5125 - val_loss: 1.0638 - val_accuracy: 0.3500\n",
            "Epoch 138/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.0427 - accuracy: 0.5625 - val_loss: 1.0626 - val_accuracy: 0.4000\n",
            "Epoch 139/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0419 - accuracy: 0.5500 - val_loss: 1.0610 - val_accuracy: 0.5500\n",
            "Epoch 140/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0452 - accuracy: 0.5125 - val_loss: 1.0569 - val_accuracy: 0.6500\n",
            "Epoch 141/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0478 - accuracy: 0.6125 - val_loss: 1.0544 - val_accuracy: 0.7000\n",
            "Epoch 142/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0420 - accuracy: 0.5625 - val_loss: 1.0541 - val_accuracy: 0.7000\n",
            "Epoch 143/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0392 - accuracy: 0.6500 - val_loss: 1.0522 - val_accuracy: 0.7000\n",
            "Epoch 144/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0359 - accuracy: 0.6375 - val_loss: 1.0526 - val_accuracy: 0.6500\n",
            "Epoch 145/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0444 - accuracy: 0.5625 - val_loss: 1.0498 - val_accuracy: 0.7000\n",
            "Epoch 146/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0399 - accuracy: 0.6250 - val_loss: 1.0465 - val_accuracy: 0.7000\n",
            "Epoch 147/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0403 - accuracy: 0.5500 - val_loss: 1.0470 - val_accuracy: 0.7000\n",
            "Epoch 148/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0370 - accuracy: 0.5875 - val_loss: 1.0452 - val_accuracy: 0.7000\n",
            "Epoch 149/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0457 - accuracy: 0.6000 - val_loss: 1.0414 - val_accuracy: 0.7000\n",
            "Epoch 150/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0278 - accuracy: 0.6375 - val_loss: 1.0400 - val_accuracy: 0.7000\n",
            "Epoch 151/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0408 - accuracy: 0.5750 - val_loss: 1.0416 - val_accuracy: 0.7000\n",
            "Epoch 152/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0324 - accuracy: 0.5875 - val_loss: 1.0375 - val_accuracy: 0.7000\n",
            "Epoch 153/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0222 - accuracy: 0.6250 - val_loss: 1.0342 - val_accuracy: 0.7000\n",
            "Epoch 154/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0338 - accuracy: 0.6000 - val_loss: 1.0363 - val_accuracy: 0.7000\n",
            "Epoch 155/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0121 - accuracy: 0.6875 - val_loss: 1.0346 - val_accuracy: 0.7000\n",
            "Epoch 156/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0257 - accuracy: 0.6125 - val_loss: 1.0342 - val_accuracy: 0.7000\n",
            "Epoch 157/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0293 - accuracy: 0.6375 - val_loss: 1.0291 - val_accuracy: 0.7000\n",
            "Epoch 158/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0251 - accuracy: 0.6000 - val_loss: 1.0292 - val_accuracy: 0.7000\n",
            "Epoch 159/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0181 - accuracy: 0.6500 - val_loss: 1.0255 - val_accuracy: 0.7000\n",
            "Epoch 160/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0170 - accuracy: 0.6500 - val_loss: 1.0239 - val_accuracy: 0.7000\n",
            "Epoch 161/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.0195 - accuracy: 0.6250 - val_loss: 1.0249 - val_accuracy: 0.7000\n",
            "Epoch 162/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.0182 - accuracy: 0.6250 - val_loss: 1.0228 - val_accuracy: 0.7000\n",
            "Epoch 163/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0197 - accuracy: 0.6500 - val_loss: 1.0163 - val_accuracy: 0.7000\n",
            "Epoch 164/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0264 - accuracy: 0.5875 - val_loss: 1.0154 - val_accuracy: 0.7000\n",
            "Epoch 165/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0098 - accuracy: 0.6375 - val_loss: 1.0094 - val_accuracy: 0.7000\n",
            "Epoch 166/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0144 - accuracy: 0.6125 - val_loss: 1.0122 - val_accuracy: 0.7000\n",
            "Epoch 167/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0110 - accuracy: 0.6375 - val_loss: 1.0087 - val_accuracy: 0.7000\n",
            "Epoch 168/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9947 - accuracy: 0.6750 - val_loss: 1.0039 - val_accuracy: 0.7000\n",
            "Epoch 169/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9932 - accuracy: 0.6500 - val_loss: 1.0024 - val_accuracy: 0.7000\n",
            "Epoch 170/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9964 - accuracy: 0.6500 - val_loss: 0.9993 - val_accuracy: 0.7000\n",
            "Epoch 171/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9989 - accuracy: 0.6500 - val_loss: 0.9988 - val_accuracy: 0.7000\n",
            "Epoch 172/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.0002 - accuracy: 0.6000 - val_loss: 0.9959 - val_accuracy: 0.7000\n",
            "Epoch 173/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9972 - accuracy: 0.6375 - val_loss: 0.9929 - val_accuracy: 0.7000\n",
            "Epoch 174/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1.0033 - accuracy: 0.6000 - val_loss: 0.9948 - val_accuracy: 0.7000\n",
            "Epoch 175/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.9863 - accuracy: 0.6750 - val_loss: 0.9884 - val_accuracy: 0.7000\n",
            "Epoch 176/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9802 - accuracy: 0.6375 - val_loss: 0.9889 - val_accuracy: 0.7000\n",
            "Epoch 177/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9963 - accuracy: 0.5875 - val_loss: 0.9847 - val_accuracy: 0.7000\n",
            "Epoch 178/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.9751 - accuracy: 0.6625 - val_loss: 0.9786 - val_accuracy: 0.7000\n",
            "Epoch 179/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9820 - accuracy: 0.6375 - val_loss: 0.9747 - val_accuracy: 0.7000\n",
            "Epoch 180/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9862 - accuracy: 0.5625 - val_loss: 0.9788 - val_accuracy: 0.7000\n",
            "Epoch 181/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9776 - accuracy: 0.6375 - val_loss: 0.9739 - val_accuracy: 0.7000\n",
            "Epoch 182/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9672 - accuracy: 0.6375 - val_loss: 0.9694 - val_accuracy: 0.7000\n",
            "Epoch 183/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9802 - accuracy: 0.6375 - val_loss: 0.9671 - val_accuracy: 0.7000\n",
            "Epoch 184/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9634 - accuracy: 0.6500 - val_loss: 0.9679 - val_accuracy: 0.7000\n",
            "Epoch 185/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9698 - accuracy: 0.6125 - val_loss: 0.9654 - val_accuracy: 0.7000\n",
            "Epoch 186/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9672 - accuracy: 0.6375 - val_loss: 0.9669 - val_accuracy: 0.7000\n",
            "Epoch 187/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9772 - accuracy: 0.6000 - val_loss: 0.9637 - val_accuracy: 0.7000\n",
            "Epoch 188/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9638 - accuracy: 0.6500 - val_loss: 0.9596 - val_accuracy: 0.7000\n",
            "Epoch 189/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9727 - accuracy: 0.6000 - val_loss: 0.9557 - val_accuracy: 0.7000\n",
            "Epoch 190/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9489 - accuracy: 0.6625 - val_loss: 0.9555 - val_accuracy: 0.7000\n",
            "Epoch 191/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9705 - accuracy: 0.6000 - val_loss: 0.9512 - val_accuracy: 0.7000\n",
            "Epoch 192/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9653 - accuracy: 0.6125 - val_loss: 0.9491 - val_accuracy: 0.7000\n",
            "Epoch 193/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9721 - accuracy: 0.6125 - val_loss: 0.9464 - val_accuracy: 0.7000\n",
            "Epoch 194/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9646 - accuracy: 0.6125 - val_loss: 0.9462 - val_accuracy: 0.7000\n",
            "Epoch 195/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.9374 - accuracy: 0.6625 - val_loss: 0.9429 - val_accuracy: 0.7000\n",
            "Epoch 196/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9295 - accuracy: 0.6750 - val_loss: 0.9415 - val_accuracy: 0.7000\n",
            "Epoch 197/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9222 - accuracy: 0.6500 - val_loss: 0.9341 - val_accuracy: 0.7000\n",
            "Epoch 198/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9598 - accuracy: 0.5750 - val_loss: 0.9304 - val_accuracy: 0.7000\n",
            "Epoch 199/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9299 - accuracy: 0.6625 - val_loss: 0.9284 - val_accuracy: 0.7000\n",
            "Epoch 200/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9325 - accuracy: 0.6375 - val_loss: 0.9199 - val_accuracy: 0.7000\n",
            "Epoch 201/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9240 - accuracy: 0.6625 - val_loss: 0.9157 - val_accuracy: 0.7000\n",
            "Epoch 202/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9425 - accuracy: 0.5375 - val_loss: 0.9184 - val_accuracy: 0.7000\n",
            "Epoch 203/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9278 - accuracy: 0.6125 - val_loss: 0.9142 - val_accuracy: 0.7000\n",
            "Epoch 204/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9292 - accuracy: 0.6125 - val_loss: 0.9124 - val_accuracy: 0.7000\n",
            "Epoch 205/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9258 - accuracy: 0.6250 - val_loss: 0.9145 - val_accuracy: 0.7000\n",
            "Epoch 206/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9211 - accuracy: 0.6500 - val_loss: 0.9109 - val_accuracy: 0.7000\n",
            "Epoch 207/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9142 - accuracy: 0.6375 - val_loss: 0.9042 - val_accuracy: 0.7000\n",
            "Epoch 208/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9188 - accuracy: 0.6375 - val_loss: 0.8941 - val_accuracy: 0.7000\n",
            "Epoch 209/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9223 - accuracy: 0.6125 - val_loss: 0.8962 - val_accuracy: 0.7000\n",
            "Epoch 210/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9152 - accuracy: 0.6125 - val_loss: 0.8910 - val_accuracy: 0.7000\n",
            "Epoch 211/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9149 - accuracy: 0.6250 - val_loss: 0.8915 - val_accuracy: 0.7000\n",
            "Epoch 212/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9091 - accuracy: 0.6125 - val_loss: 0.8852 - val_accuracy: 0.7000\n",
            "Epoch 213/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9000 - accuracy: 0.6125 - val_loss: 0.8825 - val_accuracy: 0.7000\n",
            "Epoch 214/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9225 - accuracy: 0.6250 - val_loss: 0.8869 - val_accuracy: 0.7000\n",
            "Epoch 215/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9148 - accuracy: 0.6125 - val_loss: 0.8749 - val_accuracy: 0.7000\n",
            "Epoch 216/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9266 - accuracy: 0.5625 - val_loss: 0.8762 - val_accuracy: 0.7000\n",
            "Epoch 217/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9037 - accuracy: 0.6125 - val_loss: 0.8709 - val_accuracy: 0.7000\n",
            "Epoch 218/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8990 - accuracy: 0.6375 - val_loss: 0.8685 - val_accuracy: 0.7000\n",
            "Epoch 219/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8862 - accuracy: 0.6250 - val_loss: 0.8629 - val_accuracy: 0.7000\n",
            "Epoch 220/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.8976 - accuracy: 0.6500 - val_loss: 0.8616 - val_accuracy: 0.7000\n",
            "Epoch 221/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8728 - accuracy: 0.6875 - val_loss: 0.8524 - val_accuracy: 0.7000\n",
            "Epoch 222/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.8879 - accuracy: 0.6250 - val_loss: 0.8551 - val_accuracy: 0.7000\n",
            "Epoch 223/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9028 - accuracy: 0.5875 - val_loss: 0.8573 - val_accuracy: 0.7000\n",
            "Epoch 224/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8736 - accuracy: 0.6375 - val_loss: 0.8486 - val_accuracy: 0.7000\n",
            "Epoch 225/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8714 - accuracy: 0.6375 - val_loss: 0.8464 - val_accuracy: 0.7000\n",
            "Epoch 226/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.8846 - accuracy: 0.6250 - val_loss: 0.8407 - val_accuracy: 0.7000\n",
            "Epoch 227/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.9036 - accuracy: 0.6000 - val_loss: 0.8345 - val_accuracy: 0.7000\n",
            "Epoch 228/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8731 - accuracy: 0.6125 - val_loss: 0.8310 - val_accuracy: 0.7000\n",
            "Epoch 229/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8953 - accuracy: 0.6125 - val_loss: 0.8285 - val_accuracy: 0.7000\n",
            "Epoch 230/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8540 - accuracy: 0.6500 - val_loss: 0.8247 - val_accuracy: 0.7000\n",
            "Epoch 231/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8571 - accuracy: 0.6500 - val_loss: 0.8211 - val_accuracy: 0.7000\n",
            "Epoch 232/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8413 - accuracy: 0.6750 - val_loss: 0.8269 - val_accuracy: 0.7000\n",
            "Epoch 233/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.8765 - accuracy: 0.6250 - val_loss: 0.8212 - val_accuracy: 0.7000\n",
            "Epoch 234/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.8567 - accuracy: 0.6250 - val_loss: 0.8161 - val_accuracy: 0.7000\n",
            "Epoch 235/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.8701 - accuracy: 0.6000 - val_loss: 0.8121 - val_accuracy: 0.7000\n",
            "Epoch 236/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.8324 - accuracy: 0.6375 - val_loss: 0.8075 - val_accuracy: 0.7000\n",
            "Epoch 237/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8499 - accuracy: 0.6125 - val_loss: 0.8058 - val_accuracy: 0.7000\n",
            "Epoch 238/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8309 - accuracy: 0.6750 - val_loss: 0.8030 - val_accuracy: 0.7000\n",
            "Epoch 239/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8288 - accuracy: 0.6250 - val_loss: 0.8070 - val_accuracy: 0.7000\n",
            "Epoch 240/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.8390 - accuracy: 0.6375 - val_loss: 0.7949 - val_accuracy: 0.7000\n",
            "Epoch 241/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8526 - accuracy: 0.6250 - val_loss: 0.7902 - val_accuracy: 0.7000\n",
            "Epoch 242/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8273 - accuracy: 0.6000 - val_loss: 0.7910 - val_accuracy: 0.7000\n",
            "Epoch 243/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8097 - accuracy: 0.6750 - val_loss: 0.7855 - val_accuracy: 0.7000\n",
            "Epoch 244/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8300 - accuracy: 0.6250 - val_loss: 0.7790 - val_accuracy: 0.7000\n",
            "Epoch 245/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.8332 - accuracy: 0.6125 - val_loss: 0.7751 - val_accuracy: 0.7000\n",
            "Epoch 246/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.8024 - accuracy: 0.6625 - val_loss: 0.7741 - val_accuracy: 0.7000\n",
            "Epoch 247/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8495 - accuracy: 0.6250 - val_loss: 0.7736 - val_accuracy: 0.7000\n",
            "Epoch 248/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8136 - accuracy: 0.6625 - val_loss: 0.7694 - val_accuracy: 0.7000\n",
            "Epoch 249/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8246 - accuracy: 0.6750 - val_loss: 0.7673 - val_accuracy: 0.7000\n",
            "Epoch 250/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8492 - accuracy: 0.6000 - val_loss: 0.7636 - val_accuracy: 0.7000\n",
            "Epoch 251/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7994 - accuracy: 0.6750 - val_loss: 0.7598 - val_accuracy: 0.7000\n",
            "Epoch 252/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7974 - accuracy: 0.6500 - val_loss: 0.7595 - val_accuracy: 0.7000\n",
            "Epoch 253/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8035 - accuracy: 0.6500 - val_loss: 0.7532 - val_accuracy: 0.7000\n",
            "Epoch 254/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8149 - accuracy: 0.6375 - val_loss: 0.7490 - val_accuracy: 0.7000\n",
            "Epoch 255/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.8071 - accuracy: 0.6000 - val_loss: 0.7458 - val_accuracy: 0.7000\n",
            "Epoch 256/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7949 - accuracy: 0.6500 - val_loss: 0.7514 - val_accuracy: 0.7000\n",
            "Epoch 257/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8051 - accuracy: 0.6250 - val_loss: 0.7429 - val_accuracy: 0.7000\n",
            "Epoch 258/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7900 - accuracy: 0.6625 - val_loss: 0.7376 - val_accuracy: 0.7000\n",
            "Epoch 259/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.8031 - accuracy: 0.5875 - val_loss: 0.7314 - val_accuracy: 0.7000\n",
            "Epoch 260/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8031 - accuracy: 0.6750 - val_loss: 0.7297 - val_accuracy: 0.7000\n",
            "Epoch 261/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7962 - accuracy: 0.6500 - val_loss: 0.7244 - val_accuracy: 0.7000\n",
            "Epoch 262/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7638 - accuracy: 0.6500 - val_loss: 0.7283 - val_accuracy: 0.7000\n",
            "Epoch 263/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.8114 - accuracy: 0.6375 - val_loss: 0.7175 - val_accuracy: 0.7000\n",
            "Epoch 264/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7759 - accuracy: 0.6625 - val_loss: 0.7083 - val_accuracy: 0.7000\n",
            "Epoch 265/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7870 - accuracy: 0.6500 - val_loss: 0.7055 - val_accuracy: 0.7000\n",
            "Epoch 266/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7668 - accuracy: 0.6125 - val_loss: 0.7087 - val_accuracy: 0.7000\n",
            "Epoch 267/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7431 - accuracy: 0.6500 - val_loss: 0.7076 - val_accuracy: 0.7000\n",
            "Epoch 268/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7703 - accuracy: 0.6375 - val_loss: 0.6982 - val_accuracy: 0.7000\n",
            "Epoch 269/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7478 - accuracy: 0.6250 - val_loss: 0.7041 - val_accuracy: 0.7000\n",
            "Epoch 270/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7851 - accuracy: 0.6500 - val_loss: 0.7019 - val_accuracy: 0.7000\n",
            "Epoch 271/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.8068 - accuracy: 0.5875 - val_loss: 0.6897 - val_accuracy: 0.7000\n",
            "Epoch 272/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7602 - accuracy: 0.6375 - val_loss: 0.6926 - val_accuracy: 0.7000\n",
            "Epoch 273/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7595 - accuracy: 0.6750 - val_loss: 0.6922 - val_accuracy: 0.7000\n",
            "Epoch 274/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.7321 - accuracy: 0.6875 - val_loss: 0.6789 - val_accuracy: 0.7000\n",
            "Epoch 275/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7412 - accuracy: 0.6125 - val_loss: 0.6812 - val_accuracy: 0.7000\n",
            "Epoch 276/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7551 - accuracy: 0.6375 - val_loss: 0.6781 - val_accuracy: 0.7000\n",
            "Epoch 277/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7081 - accuracy: 0.6750 - val_loss: 0.6873 - val_accuracy: 0.7000\n",
            "Epoch 278/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7382 - accuracy: 0.6875 - val_loss: 0.6788 - val_accuracy: 0.7000\n",
            "Epoch 279/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7569 - accuracy: 0.6375 - val_loss: 0.6685 - val_accuracy: 0.7000\n",
            "Epoch 280/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7588 - accuracy: 0.6500 - val_loss: 0.6650 - val_accuracy: 0.7000\n",
            "Epoch 281/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7377 - accuracy: 0.6500 - val_loss: 0.6600 - val_accuracy: 0.7000\n",
            "Epoch 282/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7253 - accuracy: 0.6375 - val_loss: 0.6607 - val_accuracy: 0.7000\n",
            "Epoch 283/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7218 - accuracy: 0.6250 - val_loss: 0.6553 - val_accuracy: 0.7000\n",
            "Epoch 284/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7207 - accuracy: 0.6500 - val_loss: 0.6535 - val_accuracy: 0.7000\n",
            "Epoch 285/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6904 - accuracy: 0.6625 - val_loss: 0.6552 - val_accuracy: 0.7000\n",
            "Epoch 286/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7234 - accuracy: 0.6250 - val_loss: 0.6575 - val_accuracy: 0.7000\n",
            "Epoch 287/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7240 - accuracy: 0.6500 - val_loss: 0.6481 - val_accuracy: 0.7000\n",
            "Epoch 288/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7312 - accuracy: 0.6000 - val_loss: 0.6498 - val_accuracy: 0.7000\n",
            "Epoch 289/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6950 - accuracy: 0.6750 - val_loss: 0.6414 - val_accuracy: 0.7000\n",
            "Epoch 290/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6984 - accuracy: 0.6750 - val_loss: 0.6344 - val_accuracy: 0.7000\n",
            "Epoch 291/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7408 - accuracy: 0.6250 - val_loss: 0.6375 - val_accuracy: 0.7000\n",
            "Epoch 292/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6890 - accuracy: 0.6625 - val_loss: 0.6257 - val_accuracy: 0.7000\n",
            "Epoch 293/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6775 - accuracy: 0.6875 - val_loss: 0.6235 - val_accuracy: 0.7000\n",
            "Epoch 294/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6964 - accuracy: 0.7000 - val_loss: 0.6204 - val_accuracy: 0.7000\n",
            "Epoch 295/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7148 - accuracy: 0.5875 - val_loss: 0.6210 - val_accuracy: 0.7000\n",
            "Epoch 296/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.6375 - val_loss: 0.6157 - val_accuracy: 0.7000\n",
            "Epoch 297/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6762 - accuracy: 0.6500 - val_loss: 0.6146 - val_accuracy: 0.7000\n",
            "Epoch 298/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7105 - accuracy: 0.6500 - val_loss: 0.6116 - val_accuracy: 0.7000\n",
            "Epoch 299/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6779 - accuracy: 0.6500 - val_loss: 0.6146 - val_accuracy: 0.7000\n",
            "Epoch 300/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6697 - accuracy: 0.6625 - val_loss: 0.6045 - val_accuracy: 0.7000\n",
            "Epoch 301/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.6625 - val_loss: 0.6027 - val_accuracy: 0.7000\n",
            "Epoch 302/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6759 - accuracy: 0.6500 - val_loss: 0.6048 - val_accuracy: 0.7000\n",
            "Epoch 303/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6729 - accuracy: 0.6750 - val_loss: 0.5952 - val_accuracy: 0.7000\n",
            "Epoch 304/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.6875 - val_loss: 0.5995 - val_accuracy: 0.7000\n",
            "Epoch 305/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6848 - accuracy: 0.6500 - val_loss: 0.5891 - val_accuracy: 0.7000\n",
            "Epoch 306/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7153 - accuracy: 0.6375 - val_loss: 0.5842 - val_accuracy: 0.7000\n",
            "Epoch 307/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6800 - accuracy: 0.6625 - val_loss: 0.5817 - val_accuracy: 0.7000\n",
            "Epoch 308/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6379 - accuracy: 0.6500 - val_loss: 0.5865 - val_accuracy: 0.7000\n",
            "Epoch 309/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6696 - accuracy: 0.6250 - val_loss: 0.5825 - val_accuracy: 0.7000\n",
            "Epoch 310/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6556 - accuracy: 0.7375 - val_loss: 0.5752 - val_accuracy: 0.7000\n",
            "Epoch 311/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6814 - accuracy: 0.7250 - val_loss: 0.5756 - val_accuracy: 0.7000\n",
            "Epoch 312/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6763 - accuracy: 0.7000 - val_loss: 0.5712 - val_accuracy: 0.7000\n",
            "Epoch 313/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6428 - accuracy: 0.6875 - val_loss: 0.5743 - val_accuracy: 0.7000\n",
            "Epoch 314/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6641 - accuracy: 0.6500 - val_loss: 0.5667 - val_accuracy: 0.7000\n",
            "Epoch 315/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6527 - accuracy: 0.6750 - val_loss: 0.5681 - val_accuracy: 0.7000\n",
            "Epoch 316/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.6375 - val_loss: 0.5621 - val_accuracy: 0.7000\n",
            "Epoch 317/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6663 - accuracy: 0.6750 - val_loss: 0.5588 - val_accuracy: 0.7000\n",
            "Epoch 318/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6653 - accuracy: 0.6625 - val_loss: 0.5551 - val_accuracy: 0.7000\n",
            "Epoch 319/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6642 - accuracy: 0.6875 - val_loss: 0.5641 - val_accuracy: 0.7000\n",
            "Epoch 320/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6463 - accuracy: 0.6875 - val_loss: 0.5588 - val_accuracy: 0.7000\n",
            "Epoch 321/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6426 - accuracy: 0.6875 - val_loss: 0.5525 - val_accuracy: 0.7000\n",
            "Epoch 322/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6205 - accuracy: 0.7125 - val_loss: 0.5487 - val_accuracy: 0.7000\n",
            "Epoch 323/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6439 - accuracy: 0.6875 - val_loss: 0.5491 - val_accuracy: 0.7000\n",
            "Epoch 324/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6079 - accuracy: 0.7500 - val_loss: 0.5454 - val_accuracy: 0.7000\n",
            "Epoch 325/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6524 - accuracy: 0.6500 - val_loss: 0.5432 - val_accuracy: 0.7000\n",
            "Epoch 326/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6345 - accuracy: 0.6750 - val_loss: 0.5395 - val_accuracy: 0.7000\n",
            "Epoch 327/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6418 - accuracy: 0.6750 - val_loss: 0.5393 - val_accuracy: 0.7000\n",
            "Epoch 328/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6599 - accuracy: 0.6750 - val_loss: 0.5419 - val_accuracy: 0.7000\n",
            "Epoch 329/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6133 - accuracy: 0.7250 - val_loss: 0.5359 - val_accuracy: 0.7000\n",
            "Epoch 330/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6242 - accuracy: 0.7375 - val_loss: 0.5391 - val_accuracy: 0.7000\n",
            "Epoch 331/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6612 - accuracy: 0.6750 - val_loss: 0.5313 - val_accuracy: 0.7000\n",
            "Epoch 332/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6648 - accuracy: 0.6500 - val_loss: 0.5250 - val_accuracy: 0.7000\n",
            "Epoch 333/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6331 - accuracy: 0.6625 - val_loss: 0.5256 - val_accuracy: 0.7000\n",
            "Epoch 334/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6228 - accuracy: 0.7000 - val_loss: 0.5280 - val_accuracy: 0.7000\n",
            "Epoch 335/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6667 - accuracy: 0.6500 - val_loss: 0.5238 - val_accuracy: 0.7000\n",
            "Epoch 336/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6087 - accuracy: 0.6750 - val_loss: 0.5178 - val_accuracy: 0.7000\n",
            "Epoch 337/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6452 - accuracy: 0.6375 - val_loss: 0.5213 - val_accuracy: 0.7000\n",
            "Epoch 338/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6074 - accuracy: 0.7250 - val_loss: 0.5164 - val_accuracy: 0.7000\n",
            "Epoch 339/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6263 - accuracy: 0.6500 - val_loss: 0.5118 - val_accuracy: 0.7000\n",
            "Epoch 340/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6320 - accuracy: 0.7125 - val_loss: 0.5122 - val_accuracy: 0.7000\n",
            "Epoch 341/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5885 - accuracy: 0.7000 - val_loss: 0.5121 - val_accuracy: 0.7000\n",
            "Epoch 342/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6192 - accuracy: 0.7250 - val_loss: 0.5038 - val_accuracy: 0.7000\n",
            "Epoch 343/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5990 - accuracy: 0.6750 - val_loss: 0.5080 - val_accuracy: 0.7000\n",
            "Epoch 344/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6396 - accuracy: 0.6750 - val_loss: 0.5005 - val_accuracy: 0.7000\n",
            "Epoch 345/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6269 - accuracy: 0.6625 - val_loss: 0.5018 - val_accuracy: 0.7000\n",
            "Epoch 346/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6567 - accuracy: 0.6625 - val_loss: 0.4985 - val_accuracy: 0.7000\n",
            "Epoch 347/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5873 - accuracy: 0.6875 - val_loss: 0.5045 - val_accuracy: 0.7000\n",
            "Epoch 348/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6153 - accuracy: 0.6500 - val_loss: 0.4994 - val_accuracy: 0.7000\n",
            "Epoch 349/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6046 - accuracy: 0.6750 - val_loss: 0.4911 - val_accuracy: 0.7000\n",
            "Epoch 350/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6031 - accuracy: 0.7000 - val_loss: 0.4907 - val_accuracy: 0.7000\n",
            "Epoch 351/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6192 - accuracy: 0.6875 - val_loss: 0.4879 - val_accuracy: 0.7000\n",
            "Epoch 352/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5985 - accuracy: 0.6625 - val_loss: 0.4870 - val_accuracy: 0.7000\n",
            "Epoch 353/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6156 - accuracy: 0.6500 - val_loss: 0.4860 - val_accuracy: 0.7000\n",
            "Epoch 354/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5934 - accuracy: 0.6750 - val_loss: 0.4835 - val_accuracy: 0.7000\n",
            "Epoch 355/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6301 - accuracy: 0.6375 - val_loss: 0.4899 - val_accuracy: 0.7000\n",
            "Epoch 356/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5922 - accuracy: 0.6750 - val_loss: 0.4811 - val_accuracy: 0.7000\n",
            "Epoch 357/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6488 - accuracy: 0.6250 - val_loss: 0.4903 - val_accuracy: 0.7000\n",
            "Epoch 358/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5900 - accuracy: 0.7000 - val_loss: 0.4810 - val_accuracy: 0.7000\n",
            "Epoch 359/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5923 - accuracy: 0.6750 - val_loss: 0.4793 - val_accuracy: 0.7000\n",
            "Epoch 360/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5900 - accuracy: 0.6250 - val_loss: 0.4732 - val_accuracy: 0.7000\n",
            "Epoch 361/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5881 - accuracy: 0.6750 - val_loss: 0.4713 - val_accuracy: 0.7000\n",
            "Epoch 362/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6083 - accuracy: 0.6875 - val_loss: 0.4703 - val_accuracy: 0.7000\n",
            "Epoch 363/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5910 - accuracy: 0.6875 - val_loss: 0.4701 - val_accuracy: 0.7000\n",
            "Epoch 364/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5788 - accuracy: 0.6875 - val_loss: 0.4659 - val_accuracy: 0.7000\n",
            "Epoch 365/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7125 - val_loss: 0.4640 - val_accuracy: 0.7000\n",
            "Epoch 366/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5444 - accuracy: 0.6875 - val_loss: 0.4615 - val_accuracy: 0.7000\n",
            "Epoch 367/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5817 - accuracy: 0.6750 - val_loss: 0.4605 - val_accuracy: 0.7000\n",
            "Epoch 368/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5725 - accuracy: 0.6625 - val_loss: 0.4596 - val_accuracy: 0.7000\n",
            "Epoch 369/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5805 - accuracy: 0.6750 - val_loss: 0.4578 - val_accuracy: 0.7000\n",
            "Epoch 370/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5979 - accuracy: 0.6625 - val_loss: 0.4553 - val_accuracy: 0.7000\n",
            "Epoch 371/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5664 - accuracy: 0.7000 - val_loss: 0.4565 - val_accuracy: 0.7000\n",
            "Epoch 372/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5640 - accuracy: 0.7125 - val_loss: 0.4618 - val_accuracy: 0.7000\n",
            "Epoch 373/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6182 - accuracy: 0.6500 - val_loss: 0.4515 - val_accuracy: 0.7000\n",
            "Epoch 374/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5690 - accuracy: 0.6625 - val_loss: 0.4503 - val_accuracy: 0.7000\n",
            "Epoch 375/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5471 - accuracy: 0.6500 - val_loss: 0.4477 - val_accuracy: 0.7000\n",
            "Epoch 376/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5665 - accuracy: 0.6625 - val_loss: 0.4464 - val_accuracy: 0.7000\n",
            "Epoch 377/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5190 - accuracy: 0.6750 - val_loss: 0.4449 - val_accuracy: 0.7000\n",
            "Epoch 378/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5341 - accuracy: 0.7000 - val_loss: 0.4457 - val_accuracy: 0.7000\n",
            "Epoch 379/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5452 - accuracy: 0.6875 - val_loss: 0.4420 - val_accuracy: 0.7000\n",
            "Epoch 380/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5729 - accuracy: 0.7000 - val_loss: 0.4405 - val_accuracy: 0.7000\n",
            "Epoch 381/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5981 - accuracy: 0.6625 - val_loss: 0.4401 - val_accuracy: 0.7000\n",
            "Epoch 382/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5427 - accuracy: 0.7375 - val_loss: 0.4382 - val_accuracy: 0.7000\n",
            "Epoch 383/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5753 - accuracy: 0.6750 - val_loss: 0.4365 - val_accuracy: 0.7000\n",
            "Epoch 384/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5822 - accuracy: 0.7125 - val_loss: 0.4344 - val_accuracy: 0.7000\n",
            "Epoch 385/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5790 - accuracy: 0.6500 - val_loss: 0.4338 - val_accuracy: 0.7000\n",
            "Epoch 386/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5597 - accuracy: 0.7000 - val_loss: 0.4313 - val_accuracy: 0.7000\n",
            "Epoch 387/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5600 - accuracy: 0.7000 - val_loss: 0.4318 - val_accuracy: 0.7000\n",
            "Epoch 388/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5732 - accuracy: 0.6500 - val_loss: 0.4297 - val_accuracy: 0.7000\n",
            "Epoch 389/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5510 - accuracy: 0.6750 - val_loss: 0.4286 - val_accuracy: 0.7000\n",
            "Epoch 390/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5829 - accuracy: 0.6375 - val_loss: 0.4293 - val_accuracy: 0.7000\n",
            "Epoch 391/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5474 - accuracy: 0.7375 - val_loss: 0.4251 - val_accuracy: 0.7000\n",
            "Epoch 392/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5223 - accuracy: 0.6875 - val_loss: 0.4236 - val_accuracy: 0.7000\n",
            "Epoch 393/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6104 - accuracy: 0.6125 - val_loss: 0.4221 - val_accuracy: 0.7000\n",
            "Epoch 394/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5929 - accuracy: 0.6500 - val_loss: 0.4235 - val_accuracy: 0.7000\n",
            "Epoch 395/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5438 - accuracy: 0.7500 - val_loss: 0.4197 - val_accuracy: 0.7000\n",
            "Epoch 396/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5467 - accuracy: 0.6750 - val_loss: 0.4184 - val_accuracy: 0.7000\n",
            "Epoch 397/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5504 - accuracy: 0.6625 - val_loss: 0.4187 - val_accuracy: 0.7000\n",
            "Epoch 398/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5517 - accuracy: 0.6750 - val_loss: 0.4157 - val_accuracy: 0.7000\n",
            "Epoch 399/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5684 - accuracy: 0.6625 - val_loss: 0.4147 - val_accuracy: 0.7000\n",
            "Epoch 400/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5524 - accuracy: 0.7000 - val_loss: 0.4130 - val_accuracy: 0.7000\n",
            "Epoch 401/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5459 - accuracy: 0.6750 - val_loss: 0.4143 - val_accuracy: 0.7000\n",
            "Epoch 402/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5291 - accuracy: 0.6750 - val_loss: 0.4111 - val_accuracy: 0.7000\n",
            "Epoch 403/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5313 - accuracy: 0.7000 - val_loss: 0.4096 - val_accuracy: 0.7500\n",
            "Epoch 404/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5380 - accuracy: 0.7000 - val_loss: 0.4096 - val_accuracy: 0.9000\n",
            "Epoch 405/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4989 - accuracy: 0.7125 - val_loss: 0.4064 - val_accuracy: 0.9000\n",
            "Epoch 406/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5017 - accuracy: 0.7375 - val_loss: 0.4059 - val_accuracy: 0.8500\n",
            "Epoch 407/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5291 - accuracy: 0.7750 - val_loss: 0.4040 - val_accuracy: 0.9000\n",
            "Epoch 408/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5300 - accuracy: 0.8250 - val_loss: 0.4026 - val_accuracy: 0.9000\n",
            "Epoch 409/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5144 - accuracy: 0.8750 - val_loss: 0.4016 - val_accuracy: 0.9000\n",
            "Epoch 410/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5740 - accuracy: 0.8250 - val_loss: 0.4013 - val_accuracy: 0.9500\n",
            "Epoch 411/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5382 - accuracy: 0.8500 - val_loss: 0.4007 - val_accuracy: 0.9500\n",
            "Epoch 412/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5374 - accuracy: 0.8750 - val_loss: 0.3982 - val_accuracy: 0.9000\n",
            "Epoch 413/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5955 - accuracy: 0.8125 - val_loss: 0.3998 - val_accuracy: 0.9500\n",
            "Epoch 414/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5160 - accuracy: 0.8875 - val_loss: 0.3962 - val_accuracy: 0.9000\n",
            "Epoch 415/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5296 - accuracy: 0.8375 - val_loss: 0.3959 - val_accuracy: 0.9500\n",
            "Epoch 416/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4982 - accuracy: 0.8625 - val_loss: 0.3936 - val_accuracy: 0.9500\n",
            "Epoch 417/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5037 - accuracy: 0.8500 - val_loss: 0.3934 - val_accuracy: 0.9500\n",
            "Epoch 418/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5466 - accuracy: 0.8625 - val_loss: 0.3907 - val_accuracy: 0.9500\n",
            "Epoch 419/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5273 - accuracy: 0.8375 - val_loss: 0.3905 - val_accuracy: 0.9500\n",
            "Epoch 420/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4904 - accuracy: 0.9000 - val_loss: 0.3898 - val_accuracy: 0.9500\n",
            "Epoch 421/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5014 - accuracy: 0.8750 - val_loss: 0.3892 - val_accuracy: 0.9500\n",
            "Epoch 422/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5022 - accuracy: 0.8750 - val_loss: 0.3860 - val_accuracy: 0.9500\n",
            "Epoch 423/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5426 - accuracy: 0.8875 - val_loss: 0.3875 - val_accuracy: 0.9500\n",
            "Epoch 424/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5187 - accuracy: 0.9125 - val_loss: 0.3825 - val_accuracy: 0.9500\n",
            "Epoch 425/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5436 - accuracy: 0.8625 - val_loss: 0.3827 - val_accuracy: 0.9500\n",
            "Epoch 426/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5722 - accuracy: 0.8250 - val_loss: 0.3821 - val_accuracy: 0.9500\n",
            "Epoch 427/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5290 - accuracy: 0.8500 - val_loss: 0.3796 - val_accuracy: 0.9500\n",
            "Epoch 428/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5474 - accuracy: 0.8375 - val_loss: 0.3790 - val_accuracy: 0.9500\n",
            "Epoch 429/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5156 - accuracy: 0.9000 - val_loss: 0.3832 - val_accuracy: 0.9000\n",
            "Epoch 430/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4615 - accuracy: 0.9000 - val_loss: 0.3793 - val_accuracy: 0.9000\n",
            "Epoch 431/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4984 - accuracy: 0.8875 - val_loss: 0.3761 - val_accuracy: 0.9500\n",
            "Epoch 432/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4992 - accuracy: 0.8250 - val_loss: 0.3725 - val_accuracy: 0.9500\n",
            "Epoch 433/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5056 - accuracy: 0.8750 - val_loss: 0.3727 - val_accuracy: 0.9500\n",
            "Epoch 434/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5336 - accuracy: 0.8625 - val_loss: 0.3708 - val_accuracy: 0.9500\n",
            "Epoch 435/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4911 - accuracy: 0.8375 - val_loss: 0.3704 - val_accuracy: 0.9500\n",
            "Epoch 436/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5152 - accuracy: 0.8750 - val_loss: 0.3690 - val_accuracy: 0.9500\n",
            "Epoch 437/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4841 - accuracy: 0.8625 - val_loss: 0.3684 - val_accuracy: 0.9500\n",
            "Epoch 438/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4836 - accuracy: 0.8500 - val_loss: 0.3666 - val_accuracy: 0.9500\n",
            "Epoch 439/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4700 - accuracy: 0.8875 - val_loss: 0.3671 - val_accuracy: 0.9500\n",
            "Epoch 440/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4892 - accuracy: 0.8750 - val_loss: 0.3654 - val_accuracy: 0.9500\n",
            "Epoch 441/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5094 - accuracy: 0.8625 - val_loss: 0.3642 - val_accuracy: 0.9500\n",
            "Epoch 442/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4889 - accuracy: 0.8875 - val_loss: 0.3626 - val_accuracy: 0.9500\n",
            "Epoch 443/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5280 - accuracy: 0.8250 - val_loss: 0.3621 - val_accuracy: 0.9500\n",
            "Epoch 444/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4896 - accuracy: 0.8750 - val_loss: 0.3612 - val_accuracy: 0.9500\n",
            "Epoch 445/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4787 - accuracy: 0.8875 - val_loss: 0.3587 - val_accuracy: 0.9500\n",
            "Epoch 446/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4921 - accuracy: 0.8750 - val_loss: 0.3573 - val_accuracy: 0.9500\n",
            "Epoch 447/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5116 - accuracy: 0.8500 - val_loss: 0.3565 - val_accuracy: 0.9500\n",
            "Epoch 448/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5171 - accuracy: 0.8250 - val_loss: 0.3555 - val_accuracy: 0.9500\n",
            "Epoch 449/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4618 - accuracy: 0.8500 - val_loss: 0.3546 - val_accuracy: 0.9500\n",
            "Epoch 450/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4831 - accuracy: 0.8500 - val_loss: 0.3532 - val_accuracy: 0.9500\n",
            "Epoch 451/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4845 - accuracy: 0.8625 - val_loss: 0.3517 - val_accuracy: 0.9500\n",
            "Epoch 452/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5096 - accuracy: 0.8625 - val_loss: 0.3522 - val_accuracy: 0.9500\n",
            "Epoch 453/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4668 - accuracy: 0.8875 - val_loss: 0.3513 - val_accuracy: 0.9500\n",
            "Epoch 454/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4732 - accuracy: 0.8250 - val_loss: 0.3484 - val_accuracy: 0.9500\n",
            "Epoch 455/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4168 - accuracy: 0.9375 - val_loss: 0.3491 - val_accuracy: 0.9500\n",
            "Epoch 456/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4762 - accuracy: 0.8875 - val_loss: 0.3454 - val_accuracy: 0.9500\n",
            "Epoch 457/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4848 - accuracy: 0.8500 - val_loss: 0.3446 - val_accuracy: 0.9500\n",
            "Epoch 458/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4706 - accuracy: 0.8875 - val_loss: 0.3435 - val_accuracy: 0.9500\n",
            "Epoch 459/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4707 - accuracy: 0.9000 - val_loss: 0.3426 - val_accuracy: 0.9500\n",
            "Epoch 460/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4869 - accuracy: 0.8375 - val_loss: 0.3413 - val_accuracy: 0.9500\n",
            "Epoch 461/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4612 - accuracy: 0.8625 - val_loss: 0.3401 - val_accuracy: 0.9500\n",
            "Epoch 462/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4691 - accuracy: 0.8875 - val_loss: 0.3422 - val_accuracy: 0.9500\n",
            "Epoch 463/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4753 - accuracy: 0.8500 - val_loss: 0.3382 - val_accuracy: 0.9500\n",
            "Epoch 464/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4598 - accuracy: 0.8625 - val_loss: 0.3384 - val_accuracy: 0.9500\n",
            "Epoch 465/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4973 - accuracy: 0.7875 - val_loss: 0.3375 - val_accuracy: 0.9500\n",
            "Epoch 466/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5181 - accuracy: 0.8000 - val_loss: 0.3346 - val_accuracy: 0.9500\n",
            "Epoch 467/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4197 - accuracy: 0.9125 - val_loss: 0.3356 - val_accuracy: 0.9500\n",
            "Epoch 468/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4538 - accuracy: 0.8500 - val_loss: 0.3351 - val_accuracy: 0.9500\n",
            "Epoch 469/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4745 - accuracy: 0.8750 - val_loss: 0.3312 - val_accuracy: 0.9500\n",
            "Epoch 470/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4487 - accuracy: 0.9125 - val_loss: 0.3301 - val_accuracy: 0.9500\n",
            "Epoch 471/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4950 - accuracy: 0.8375 - val_loss: 0.3295 - val_accuracy: 0.9500\n",
            "Epoch 472/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4654 - accuracy: 0.8750 - val_loss: 0.3292 - val_accuracy: 0.9500\n",
            "Epoch 473/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4349 - accuracy: 0.9250 - val_loss: 0.3283 - val_accuracy: 0.9500\n",
            "Epoch 474/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4726 - accuracy: 0.8000 - val_loss: 0.3273 - val_accuracy: 0.9500\n",
            "Epoch 475/500\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4734 - accuracy: 0.8500 - val_loss: 0.3260 - val_accuracy: 0.9500\n",
            "Epoch 476/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3868 - accuracy: 0.9250 - val_loss: 0.3252 - val_accuracy: 0.9500\n",
            "Epoch 477/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4409 - accuracy: 0.8875 - val_loss: 0.3230 - val_accuracy: 0.9500\n",
            "Epoch 478/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4795 - accuracy: 0.8375 - val_loss: 0.3218 - val_accuracy: 0.9500\n",
            "Epoch 479/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4726 - accuracy: 0.8625 - val_loss: 0.3210 - val_accuracy: 0.9500\n",
            "Epoch 480/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4373 - accuracy: 0.8750 - val_loss: 0.3203 - val_accuracy: 0.9500\n",
            "Epoch 481/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4275 - accuracy: 0.8875 - val_loss: 0.3248 - val_accuracy: 0.9500\n",
            "Epoch 482/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4344 - accuracy: 0.8875 - val_loss: 0.3177 - val_accuracy: 0.9500\n",
            "Epoch 483/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4307 - accuracy: 0.8625 - val_loss: 0.3177 - val_accuracy: 0.9500\n",
            "Epoch 484/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4707 - accuracy: 0.8375 - val_loss: 0.3181 - val_accuracy: 0.9500\n",
            "Epoch 485/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4674 - accuracy: 0.8625 - val_loss: 0.3169 - val_accuracy: 0.9500\n",
            "Epoch 486/500\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4554 - accuracy: 0.8375 - val_loss: 0.3155 - val_accuracy: 0.9500\n",
            "Epoch 487/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4762 - accuracy: 0.8375 - val_loss: 0.3146 - val_accuracy: 0.9500\n",
            "Epoch 488/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4486 - accuracy: 0.8625 - val_loss: 0.3140 - val_accuracy: 0.9500\n",
            "Epoch 489/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4819 - accuracy: 0.8375 - val_loss: 0.3123 - val_accuracy: 0.9500\n",
            "Epoch 490/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4567 - accuracy: 0.8875 - val_loss: 0.3142 - val_accuracy: 0.9500\n",
            "Epoch 491/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4406 - accuracy: 0.8500 - val_loss: 0.3109 - val_accuracy: 0.9500\n",
            "Epoch 492/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4068 - accuracy: 0.9250 - val_loss: 0.3112 - val_accuracy: 0.9500\n",
            "Epoch 493/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4409 - accuracy: 0.8625 - val_loss: 0.3093 - val_accuracy: 0.9500\n",
            "Epoch 494/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4725 - accuracy: 0.8375 - val_loss: 0.3077 - val_accuracy: 0.9500\n",
            "Epoch 495/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4317 - accuracy: 0.8375 - val_loss: 0.3081 - val_accuracy: 0.9500\n",
            "Epoch 496/500\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4215 - accuracy: 0.8750 - val_loss: 0.3096 - val_accuracy: 0.9500\n",
            "Epoch 497/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4530 - accuracy: 0.8375 - val_loss: 0.3086 - val_accuracy: 0.9500\n",
            "Epoch 498/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4883 - accuracy: 0.8500 - val_loss: 0.3042 - val_accuracy: 0.9500\n",
            "Epoch 499/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4140 - accuracy: 0.9125 - val_loss: 0.3040 - val_accuracy: 0.9500\n",
            "Epoch 500/500\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4606 - accuracy: 0.8500 - val_loss: 0.3016 - val_accuracy: 0.9500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3499 - accuracy: 0.9400\n",
            "test_loss:  0.34988147020339966\n",
            "test_acc:  0.9399999976158142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVZ2sf6Gb5Y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e38cf41-0a75-4e35-dd9e-e2399d9db2c3"
      },
      "source": [
        "hist.history.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Hh1NwcAwLtVw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "0c382e5d-b905-41b2-f887-007cb45390d0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0LNN770LtVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61e14ea-34d9-4170-bbbd-b8843fbb75db"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_75 (Dense)            (None, 9)                 45        \n",
            "                                                                 \n",
            " activation_75 (Activation)  (None, 9)                 0         \n",
            "                                                                 \n",
            " dropout_50 (Dropout)        (None, 9)                 0         \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 4)                 40        \n",
            "                                                                 \n",
            " activation_76 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dropout_51 (Dropout)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 3)                 15        \n",
            "                                                                 \n",
            " activation_77 (Activation)  (None, 3)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 100\n",
            "Trainable params: 100\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap06VxZI5Aog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e774b88-8a2e-4802-a411-2e2dc820d6fd"
      },
      "source": [
        "model.get_weights()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.6709025 , -0.74436474,  0.30756164, -0.1685059 , -0.39201078,\n",
              "          0.90272593, -0.37256438, -0.5212559 ,  0.07052556],\n",
              "        [-0.17333336,  1.4122282 , -0.64769596,  1.3404028 ,  0.05635369,\n",
              "         -1.110277  , -0.21234795, -0.17553395, -0.02852893],\n",
              "        [ 1.6093391 , -2.0588076 ,  2.4805236 , -2.625135  ,  0.03244102,\n",
              "          2.4678106 ,  0.617988  ,  0.36674166, -2.6640904 ],\n",
              "        [ 2.9539652 , -2.0647633 ,  2.3787396 , -1.8309871 , -0.44891906,\n",
              "          2.8188887 ,  0.39295876,  0.06722617, -3.1325953 ]],\n",
              "       dtype=float32),\n",
              " array([-0.04537445,  0.10563427, -0.02282317,  0.08382259,  0.        ,\n",
              "        -0.04712012,  0.        ,  0.        ,  0.10747746], dtype=float32),\n",
              " array([[-0.14719035,  2.344687  , -1.8600905 , -1.5195205 ],\n",
              "        [ 0.5091382 , -2.2095263 ,  1.4860299 ,  2.9018385 ],\n",
              "        [-0.05279264,  2.3895886 , -0.93682927, -0.6967635 ],\n",
              "        [-0.12711303, -2.2522523 ,  2.665402  ,  2.640365  ],\n",
              "        [ 0.42077374,  0.40466237, -0.6216826 ,  0.5311601 ],\n",
              "        [-0.12838931,  3.165545  , -2.1587121 , -2.326334  ],\n",
              "        [ 0.38336134, -0.45586511, -0.04829919, -0.6500921 ],\n",
              "        [ 0.0868541 , -0.21720618,  0.26384598,  0.62871206],\n",
              "        [ 0.06402886, -0.44572333,  1.3487387 ,  2.478456  ]],\n",
              "       dtype=float32),\n",
              " array([-0.05638431,  0.40205568,  0.35075817,  0.3263065 ], dtype=float32),\n",
              " array([[-0.80786276, -0.85131764, -0.52728206],\n",
              "        [ 0.87669575, -3.7264848 ,  0.48017234],\n",
              "        [-3.0992117 ,  1.2247998 , -1.2049773 ],\n",
              "        [-3.0238833 ,  2.3464215 , -0.13740411]], dtype=float32),\n",
              " array([ 0.30507562, -0.89466983,  0.19787364], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "nhPK4V6nLtVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d52b16-ba4a-468a-c85c-201af9449fa8"
      },
      "source": [
        "model.save(dir+\"dnn_bmi.h5\")\n",
        "print(\"Saved model to disk.\")\n",
        "\n",
        "from numpy import loadtxt\n",
        "from tensorflow.python.keras.models import load_model\n",
        "\n",
        "# 저장된 모델 읽어오기\n",
        "loaded_model = load_model(dir+\"dnn_bmi.h5\")\n",
        "model.summary()\n",
        "\n",
        "# 모델을 평가하기\n",
        "score = model.evaluate(X_test, y_test)\n",
        "print('test_loss: ', score[0])\n",
        "print('test_acc: ', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to disk.\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_75 (Dense)            (None, 9)                 45        \n",
            "                                                                 \n",
            " activation_75 (Activation)  (None, 9)                 0         \n",
            "                                                                 \n",
            " dropout_50 (Dropout)        (None, 9)                 0         \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 4)                 40        \n",
            "                                                                 \n",
            " activation_76 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dropout_51 (Dropout)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 3)                 15        \n",
            "                                                                 \n",
            " activation_77 (Activation)  (None, 3)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 100\n",
            "Trainable params: 100\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.3499 - accuracy: 0.9400\n",
            "test_loss:  0.34988147020339966\n",
            "test_acc:  0.9399999976158142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8sQr21XLtVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b407307-2d59-46f6-acc3-88acd5c560aa"
      },
      "source": [
        "# X_test 샘플들의 클래스 예측하기\n",
        "y_prob = model.predict(X_test)    # X_test의 출력값 확인하기\n",
        "print(y_prob)\n",
        "\n",
        "y_class = y_prob.argmax(axis=-1)  # X_test의 클래스 예측하기\n",
        "y_class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff0f4c5dcb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 4ms/step\n",
            "[[1.7402085e-03 9.1943759e-01 7.8822128e-02]\n",
            " [6.3834584e-01 8.8611891e-04 3.6076808e-01]\n",
            " [6.2006176e-01 1.8612890e-03 3.7807703e-01]\n",
            " [8.4622681e-02 2.8942871e-01 6.2594849e-01]\n",
            " [6.4028579e-01 8.0802693e-04 3.5890627e-01]\n",
            " [7.4686686e-04 9.5324796e-01 4.6005249e-02]\n",
            " [3.5457083e-04 9.6975106e-01 2.9894302e-02]\n",
            " [5.1748269e-04 9.6260452e-01 3.6877841e-02]\n",
            " [4.8693672e-01 6.6018808e-03 5.0646144e-01]\n",
            " [6.2689966e-01 1.5129700e-03 3.7158734e-01]\n",
            " [3.7420136e-01 1.7711952e-02 6.0808665e-01]\n",
            " [7.5100490e-04 9.5298761e-01 4.6261329e-02]\n",
            " [2.3863767e-01 6.0049709e-02 7.0131260e-01]\n",
            " [9.4747043e-04 9.4529498e-01 5.3757392e-02]\n",
            " [4.4961628e-01 8.9079961e-03 5.4147571e-01]\n",
            " [6.3124454e-01 1.2373490e-03 3.6751810e-01]\n",
            " [5.4140443e-01 4.0963320e-03 4.5449927e-01]\n",
            " [2.4812642e-01 5.4144856e-02 6.9772869e-01]\n",
            " [2.2533381e-01 6.7605019e-02 7.0706117e-01]\n",
            " [6.1630312e-04 9.5862734e-01 4.0756445e-02]\n",
            " [7.5814733e-04 9.5301569e-01 4.6226211e-02]\n",
            " [7.2946446e-04 9.5401412e-01 4.5256380e-02]\n",
            " [6.3203293e-01 1.1926905e-03 3.6677432e-01]\n",
            " [6.1356014e-01 1.9835753e-03 3.8445628e-01]\n",
            " [6.2838423e-01 1.4129054e-03 3.7020287e-01]\n",
            " [5.5903872e-04 9.6110499e-01 3.8335964e-02]\n",
            " [3.5096976e-01 2.1218661e-02 6.2781149e-01]\n",
            " [6.4779633e-01 5.6309701e-04 3.5164046e-01]\n",
            " [2.2183990e-01 7.0663311e-02 7.0749676e-01]\n",
            " [5.1936938e-04 9.6277076e-01 3.6709879e-02]\n",
            " [3.0360764e-01 3.3843622e-02 6.6254872e-01]\n",
            " [2.6681083e-01 4.6707645e-02 6.8648154e-01]\n",
            " [9.5504284e-04 9.4546503e-01 5.3579934e-02]\n",
            " [7.2481669e-04 9.5397419e-01 4.5300905e-02]\n",
            " [4.8299196e-01 6.8100970e-03 5.1019794e-01]\n",
            " [4.7058472e-01 7.5509064e-03 5.2186441e-01]\n",
            " [6.4214218e-01 7.3944760e-04 3.5711831e-01]\n",
            " [4.5468688e-01 8.8378889e-03 5.3647518e-01]\n",
            " [5.3411317e-01 4.3755090e-03 4.6151131e-01]\n",
            " [6.3863927e-01 8.7386131e-04 3.6048681e-01]\n",
            " [6.1717278e-01 1.9544612e-03 3.8087267e-01]\n",
            " [7.1414257e-04 9.5458597e-01 4.4699792e-02]\n",
            " [1.6043648e-01 1.2798111e-01 7.1158248e-01]\n",
            " [6.2899894e-01 1.3733149e-03 3.6962780e-01]\n",
            " [4.7430322e-01 7.1648173e-03 5.1853186e-01]\n",
            " [1.9118668e-03 9.1534227e-01 8.2745872e-02]\n",
            " [3.0087307e-01 3.4203272e-02 6.6492367e-01]\n",
            " [6.2732011e-01 1.4839796e-03 3.7119594e-01]\n",
            " [6.2754899e-01 1.4684156e-03 3.7098262e-01]\n",
            " [7.5884047e-04 9.5279551e-01 4.6445642e-02]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 2, 0, 1, 1, 1, 2, 0, 2, 1, 2, 1, 2, 0, 0, 2, 2, 1, 1, 1,\n",
              "       0, 0, 0, 1, 2, 0, 2, 1, 2, 2, 1, 1, 2, 2, 0, 2, 0, 0, 0, 1, 2, 0,\n",
              "       2, 1, 2, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34Xez1yYLtV0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "751ead25-a0cc-4857-fa1b-e0ea3ec54658"
      },
      "source": [
        "# 새로운 샘플의 클래스 예측하기\n",
        "X_new = [80, 175]\n",
        "X_new[0]/=100; X_new[1]/=200\n",
        "print(X_new)\n",
        "\n",
        "y_prob = model.predict([X_new]) # X_new의 출력값 확인하기\n",
        "y_pred = y_prob.argmax()        # X_new의 클래스 예측하기\n",
        "print(y_prob, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8, 0.875]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-215-f18f39fa9e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# X_new의 출력값 확인하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# X_new의 클래스 예측하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1845, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1834, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1823, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1791, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_25\" is incompatible with the layer: expected shape=(None, 4), found shape=(None, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDjOYxUzLtVz"
      },
      "source": [
        "type(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOdX9FRmLtVz"
      },
      "source": [
        "X_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaOIG8Kaovrc"
      },
      "source": [
        "X_test.head(5).index.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfXV29CxuGk0"
      },
      "source": [
        "X_test0 = X_test.loc[15000]\n",
        "print(X_test0); print()\n",
        "\n",
        "X_test_li = list(X_test0)\n",
        "y_prob = model.predict([X_test_li])  # model.predict([[0.69, 0.55]])\n",
        "y_pred = y_prob.argmax()\n",
        "print(y_prob, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flB-FoC2LtV0"
      },
      "source": [
        "def predict_bmi(X_new):\n",
        "  y_prob = model.predict([X_new])\n",
        "  y_pred = y_prob.argmax()\n",
        "  print(X_new, y_prob, y_pred, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMJdeu5nLtV0"
      },
      "source": [
        "X_mean = [X_train[\"height\"].mean(), X_train[\"weight\"].mean()]\n",
        "X_min = [X_train[\"height\"].min(), X_train[\"weight\"].min()]\n",
        "X_max = [X_train[\"height\"].max(), X_train[\"weight\"].max()]\n",
        "X_min_max = [X_train[\"height\"].min(), X_train[\"weight\"].max()]\n",
        "X_max_min = [X_train[\"height\"].max(), X_train[\"weight\"].min()]\n",
        "\n",
        "predict_bmi(X_mean)\n",
        "predict_bmi(X_min)\n",
        "predict_bmi(X_max)\n",
        "predict_bmi(X_min_max)\n",
        "predict_bmi(X_max_min)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}